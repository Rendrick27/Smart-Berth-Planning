{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.361027Z",
     "start_time": "2023-06-01T13:27:23.763744Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyproj\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m block_diag\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscienceplots\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from kalman import KalmanFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", extrapolate=False in most cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.369988Z",
     "start_time": "2023-06-01T13:27:24.362013Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolate_voyage_vars(voyage_data: pd.DataFrame, quay_point):\n",
    "\n",
    "    interpolated_data = voyage_data[[\"x\", \"y\"]].interpolate(method=\"linear\", axis=0)\n",
    "\n",
    "    # convert position\n",
    "    transformer_meterstodeg = Transformer.from_crs(crs_from=3857, crs_to=4326)\n",
    "    lat, lon = transformer_meterstodeg.transform(interpolated_data[\"x\"].to_numpy(),\n",
    "                                                 interpolated_data[\"y\"].to_numpy())\n",
    "    interpolated_data[\"LON\"] = lon\n",
    "    interpolated_data[\"LAT\"] = lat\n",
    "\n",
    "    interpolated_data_shift1 = interpolated_data.shift(1)\n",
    "    interpolated_data = interpolated_data.assign(SOG=voyage_data[\"SOG\"],\n",
    "                                                 COG=voyage_data[\"COG\"],\n",
    "                                                 distanceToPort=voyage_data[\"distanceToPort\"])\n",
    "\n",
    "    # fill speed over ground (euclidean_distance/sampling interval)\n",
    "    sog = np.sqrt((interpolated_data[\"x\"] - interpolated_data_shift1[\"x\"])**2\n",
    "                  + (interpolated_data[\"y\"] - interpolated_data_shift1[\"y\"])**2) / 120 \\\n",
    "          / 0.5144444 # m/s to knots\n",
    "    interpolated_data[\"SOG\"].fillna(sog, inplace=True)\n",
    "\n",
    "    # fill course over ground (angle in relation to the true North)\n",
    "    cog = np.arctan2(interpolated_data[\"x\"] - interpolated_data_shift1[\"x\"],\n",
    "                     interpolated_data[\"y\"] - interpolated_data_shift1[\"y\"]) % (2 * np.pi) \\\n",
    "          * 180 / np.pi # from radians to degrees\n",
    "    interpolated_data[\"COG\"].fillna(cog, inplace=True)\n",
    "    \n",
    "    # fill distance to port\n",
    "    dist = np.sqrt((interpolated_data[\"x\"] - quay_point[0])**2\n",
    "                   + (interpolated_data[\"y\"] - quay_point[1])**2) \\\n",
    "           * 0.000539956803 # meters to nautical miles\n",
    "    interpolated_data[\"distanceToPort\"].fillna(dist, inplace=True)\n",
    "\n",
    "    # remove positions in meters\n",
    "    interpolated_data.drop(columns=[\"x\", \"y\"], inplace=True)\n",
    "\n",
    "    return interpolated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.409030Z",
     "start_time": "2023-06-01T13:27:24.381105Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cv_model_2d(dt, accel, meas_noise):\n",
    "\n",
    "    F = np.eye(4)\n",
    "    F[0, 1] = F[2, 3] = dt\n",
    "    H = np.zeros((2,4))\n",
    "    H[0,0] = H[1,2] = 1\n",
    "\n",
    "    block = np.array([[dt**4/4, dt**3/2],\n",
    "                      [dt**3/2, dt**2]])\n",
    "    Q = block_diag(block, block) * (accel**2)\n",
    "\n",
    "    R = np.eye(2) * (meas_noise ** 2)\n",
    "    P = np.eye(4) * 0.01\n",
    "\n",
    "    return F, H, Q, R, P\n",
    "\n",
    "def extrapolate_voyage_vars(voyage_data, quay_point):\n",
    "\n",
    "    # Get KF measures and AIS messages in different arrays\n",
    "    kf_measures = voyage_data[[\"x\", \"vx\", \"y\", \"vy\"]].values\n",
    "    voyage_vars = voyage_data[[\"LON\", \"LAT\", \"SOG\", \"COG\", \"distanceToPort\"]].values\n",
    "\n",
    "    # Initialize Kalman Filter\n",
    "    F, H, Q, R, P = cv_model_2d(120, 0.001, 10)\n",
    "    x0 = kf_measures[0, :, np.newaxis]\n",
    "    kf = KalmanFilter(x0, F, None, H, Q, R, P)\n",
    "\n",
    "    # Save P for reset\n",
    "    P = kf.P\n",
    "\n",
    "    # Variables\n",
    "    reset = False\n",
    "    extrapolated_voyage_vars = np.zeros(shape=(voyage_data.shape[0], 5)) # array of predictions\n",
    "\n",
    "    # filter the voyage data\n",
    "    for i in range(1, kf_measures.shape[0]):\n",
    "\n",
    "        if reset and np.isnan(kf_measures[i,0]): # if there is no info\n",
    "            continue\n",
    "        elif reset: # if there is info and KF needs reset\n",
    "            kf.x = kf_measures[i, :, np.newaxis]\n",
    "            kf.P = P\n",
    "\n",
    "        kf.predict()\n",
    "\n",
    "        # current measure available for correction\n",
    "        if ~np.isnan(kf_measures[i, 0]):\n",
    "            kf.update(kf_measures[i, [0, 2], np.newaxis])\n",
    "            continue\n",
    "\n",
    "        #past_x, past_y = kf_measures[i-1, [0, 2]]\n",
    "        x, y = kf.x[0, 0], kf.x[2, 0]\n",
    "        vel = np.sqrt((kf.x[0, 0] - kf_measures[i-1, 0]) ** 2 + (kf.x[2, 0] - kf_measures[i-1, 2]) ** 2) / 120\n",
    "        cog = np.arctan2(kf.x[0, 0] - kf_measures[i-1, 0], kf.x[2, 0] - kf_measures[i-1, 2]) % (2 * np.pi)\n",
    "        dist = np.sqrt((kf.x[0, 0] - quay_point[0]) ** 2 + (kf.x[2, 0] - quay_point[1]) ** 2)\n",
    "        extrapolated_voyage_vars[i, :] = np.array([x, y, vel, cog, dist])\n",
    "        reset = True\n",
    "\n",
    "    extrapolated_voyage_vars[np.isnan(extrapolated_voyage_vars[:,0])] = np.NaN\n",
    "\n",
    "    transformer = Transformer.from_crs(3857, 4326)\n",
    "    lat, lon = transformer.transform(extrapolated_voyage_vars[:, 0], extrapolated_voyage_vars[:, 1])\n",
    "    extrapolated_voyage_vars[:, 0] = lon\n",
    "    extrapolated_voyage_vars[:, 1] = lat\n",
    "    extrapolated_voyage_vars[:, 2] /= 0.5144444\n",
    "    extrapolated_voyage_vars[:, 3] *= 180 / np.pi\n",
    "    extrapolated_voyage_vars[:, 4] *= 0.000539956803\n",
    "\n",
    "    # add existing AIS messages to extrapolated voyage\n",
    "    extrapolated_voyage_vars[~np.isnan(voyage_vars[:,0])] = voyage_vars[~np.isnan(voyage_vars[:,0])]\n",
    "\n",
    "    # add variables to voyage data\n",
    "    extrapolated_voyage_vars = pd.DataFrame(data=extrapolated_voyage_vars,\n",
    "                                            columns=[\"LON\", \"LAT\", \"SOG\", \"COG\", \"distanceToPort\"])\n",
    "\n",
    "    return extrapolated_voyage_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.409445Z",
     "start_time": "2023-06-01T13:27:24.384380Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lagged_voyage_vars(voyage_vars, delays):\n",
    "\n",
    "    voyage_vars_lagged = voyage_vars.copy()\n",
    "\n",
    "    for delay in delays:\n",
    "        voyage_vars_lag = voyage_vars.shift(delay)\n",
    "\n",
    "        voyage_vars_lag.columns += \"_lag\" + str(delay)\n",
    "        voyage_vars_lagged = pd.concat((voyage_vars_lagged, voyage_vars_lag), axis=1)\n",
    "\n",
    "    return voyage_vars_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.409681Z",
     "start_time": "2023-06-01T13:27:24.389213Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complete_vars(arr, n_feats, total_cols):\n",
    "\n",
    "    voyage_lags = arr[:total_cols]\n",
    "    voyage_interp_lags = arr[total_cols:total_cols*2]\n",
    "    voyage_extrap_lags = arr[total_cols*2:total_cols*3]\n",
    "\n",
    "    non_nan_indexes = np.argwhere(~np.isnan(voyage_lags))\n",
    "    if non_nan_indexes.shape[0] == 0 or non_nan_indexes[0, 0] == 0:\n",
    "        return voyage_lags\n",
    "\n",
    "    interp_end = non_nan_indexes[0, 0]\n",
    "    voyage_lags[interp_end:] = voyage_interp_lags[interp_end:]\n",
    "    voyage_lags[interp_end-n_feats:interp_end] = \\\n",
    "        voyage_extrap_lags[interp_end-n_feats:interp_end]\n",
    "\n",
    "    return voyage_lags\n",
    "\n",
    "def get_stacked_voyage_vars_lagged(voyage_vars, delays, extrapolate, quay_point):\n",
    "\n",
    "    voyage_vars_lagged = get_lagged_voyage_vars(voyage_vars[[\"LON\", \"LAT\", \"SOG\", \"COG\", \"distanceToPort\"]],\n",
    "                                                delays=delays)\n",
    "    voyage_interp = interpolate_voyage_vars(voyage_vars, quay_point)\n",
    "    voyage_interp_lagged = get_lagged_voyage_vars(voyage_interp, delays=delays)\n",
    "\n",
    "    if extrapolate:\n",
    "        voyage_extrap = extrapolate_voyage_vars(voyage_vars, quay_point)\n",
    "        voyage_extrap_lagged = get_lagged_voyage_vars(voyage_extrap, delays=delays)\n",
    "\n",
    "        voyage_vars_lagged_stacked = np.hstack((voyage_vars_lagged.values,\n",
    "                                                voyage_interp_lagged.values,\n",
    "                                                voyage_extrap_lagged.values))\n",
    "    else:\n",
    "        voyage_with_outdated_samples = voyage_vars.loc[:, [\"LON\", \"LAT\", \"SOG\", \"COG\", \"distanceToPort\"]].ffill(limit=1)\n",
    "        voyage_with_outdated_samples_lagged = get_lagged_voyage_vars(voyage_with_outdated_samples, delays=delays)\n",
    "        voyage_vars_lagged_stacked = np.hstack((voyage_vars_lagged.values,\n",
    "                                                voyage_interp_lagged.values,\n",
    "                                                voyage_with_outdated_samples_lagged.values))\n",
    "\n",
    "    return voyage_vars_lagged_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.409888Z",
     "start_time": "2023-06-01T13:27:24.400570Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lagged_cols(columns: np.array, delays):\n",
    "    columns = np.array(columns)\n",
    "    final_columns = columns.copy()\n",
    "    for delay in delays:\n",
    "        final_columns = np.hstack((final_columns, np.char.add(columns, \"_lag\" + str(delay))))\n",
    "    return final_columns\n",
    "\n",
    "def get_lagged_vars(data: pd.DataFrame, delays, online, fill, extrapolate, quay_point):\n",
    "\n",
    "    if not fill:\n",
    "        return data.groupby(level=0, sort=False, group_keys=False)\\\n",
    "            .apply(lambda x: get_lagged_voyage_vars(x, delays))\n",
    "\n",
    "    if not online:\n",
    "        return data.groupby(level=0, sort=False, group_keys=False)\\\n",
    "            .apply(lambda x: get_lagged_voyage_vars(interpolate_voyage_vars(x, quay_point), delays))\n",
    "\n",
    "    stacked_voyage_vars_lagged =  data.groupby(level=0, sort=False, group_keys=False)\\\n",
    "        .apply(lambda x: get_stacked_voyage_vars_lagged(x, delays, extrapolate, quay_point))\n",
    "    \n",
    "    stacked_voyage_vars_lagged = np.vstack(stacked_voyage_vars_lagged.values)\n",
    "\n",
    "    n_feats = int(stacked_voyage_vars_lagged.shape[1] / 3 / (delays[-1]+1))\n",
    "    voyage_vars_lagged_filled = np.apply_along_axis(lambda x: complete_vars(x,\n",
    "                                                                            n_feats,\n",
    "                                                                            n_feats * (delays[-1]+1)),\n",
    "                                                    axis=1,\n",
    "                                                    arr=stacked_voyage_vars_lagged)\n",
    "\n",
    "    voyage_vars_lagged_filled = pd.DataFrame(data=voyage_vars_lagged_filled,\n",
    "                                             columns=get_lagged_cols([\"LON\", \"LAT\",\n",
    "                                                                      \"SOG\", \"COG\",\n",
    "                                                                      \"distanceToPort\"], delays))\n",
    "\n",
    "    return voyage_vars_lagged_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T13:27:24.410102Z",
     "start_time": "2023-06-01T13:27:24.405757Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lagged_io(lagged_vars: pd.DataFrame, target_var: pd.Series, n_feats, max_delay):\n",
    "\n",
    "    target_var.interpolate(method=\"linear\", inplace=True)\n",
    "    lagged_io = pd.concat((lagged_vars.iloc[:, :n_feats*(max_delay+1)], target_var), axis=1)\n",
    "    lagged_io.loc[lagged_io.iloc[:, :-1].isna().any(axis=1), target_var.name] = np.NaN\n",
    "\n",
    "    return lagged_io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miami Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:06:32.181816Z",
     "start_time": "2023-05-31T00:06:30.405058Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train set -> Linear Interpolation\n",
    "train = pd.read_csv(\"./data/3_miami_train_to_fill.csv\")\n",
    "train[\"BaseDateTime\"] = pd.to_datetime(train[\"BaseDateTime\"])\n",
    "train.set_index([\"voyage_id\", \"BaseDateTime\"], drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:06:32.192280Z",
     "start_time": "2023-05-31T00:06:32.168185Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:06:34.237227Z",
     "start_time": "2023-05-31T00:06:32.185920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resample to 2 minutes intervals by taking the meane very 2 minutes\n",
    "train_resampled = train.groupby(level=0, sort=False)\\\n",
    "            .resample(\"2T\", level=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:06:34.254540Z",
     "start_time": "2023-05-31T00:06:34.235913Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(train_resampled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:30:16.316675Z",
     "start_time": "2023-05-31T17:30:16.301463Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = Transformer.from_crs(4326, 3857)\n",
    "quay_point = transformer.transform(25.77, -80.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:30:17.838304Z",
     "start_time": "2023-05-31T17:30:17.823914Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delays = np.arange(1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:06:44.041977Z",
     "start_time": "2023-05-31T00:06:34.282187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vars_lagged = get_lagged_vars(train_resampled[[\"LON\", \"LAT\",\n",
    "                                                     \"SOG\", \"COG\",\n",
    "                                                     \"distanceToPort\"]],\n",
    "                                    delays=delays,\n",
    "                                    online=False, fill=False, quay_point=None, extrapolate=False)\n",
    "train_nohistory = get_lagged_io(train_vars_lagged,\n",
    "                                train_resampled[\"remainingVoyageTime\"],\n",
    "                                n_feats=5, max_delay=0)\n",
    "train_10min = get_lagged_io(train_vars_lagged,\n",
    "                            train_resampled[\"remainingVoyageTime\"],\n",
    "                            n_feats=5, max_delay=5)\n",
    "train_20min = get_lagged_io(train_vars_lagged,\n",
    "                            train_resampled[\"remainingVoyageTime\"],\n",
    "                            n_feats=5, max_delay=10)\n",
    "train_30min = get_lagged_io(train_vars_lagged,\n",
    "                            train_resampled[\"remainingVoyageTime\"],\n",
    "                            n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:44:33.923199Z",
     "start_time": "2023-05-31T00:44:13.006168Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vars_lagged_filled = get_lagged_vars(train_resampled[[\"LON\", \"LAT\",\n",
    "                                                            \"SOG\", \"COG\",\n",
    "                                                            \"distanceToPort\",\n",
    "                                                            \"x\", \"y\"]],\n",
    "                                           delays=delays,\n",
    "                                           online=False, fill=True, quay_point=quay_point, extrapolate=False)\n",
    "train_filled_nohistory = get_lagged_io(train_vars_lagged_filled,\n",
    "                                       train_resampled[\"remainingVoyageTime\"],\n",
    "                                       n_feats=5, max_delay=0)\n",
    "train_filled_10min = get_lagged_io(train_vars_lagged_filled,\n",
    "                                   train_resampled[\"remainingVoyageTime\"],\n",
    "                                   n_feats=5, max_delay=5)\n",
    "train_filled_20min = get_lagged_io(train_vars_lagged_filled,\n",
    "                                   train_resampled[\"remainingVoyageTime\"],\n",
    "                                   n_feats=5, max_delay=10)\n",
    "train_filled_30min = get_lagged_io(train_vars_lagged_filled,\n",
    "                                   train_resampled[\"remainingVoyageTime\"],\n",
    "                                   n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create folder if dont exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:39.950004Z",
     "start_time": "2023-05-31T00:44:57.980419Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the paths for each dataset\n",
    "paths = [\n",
    "    \"./data/final_miami_datasets/no_history/\",\n",
    "    \"./data/final_miami_datasets/10min_history/\",\n",
    "    \"./data/final_miami_datasets/20min_history/\",\n",
    "    \"./data/final_miami_datasets/30min_history/\"\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in paths:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Save each dataset to its respective folder\n",
    "train_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/no_history/train.csv\", index=True)\n",
    "\n",
    "train_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/10min_history/train.csv\", index=True)\n",
    "\n",
    "train_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/20min_history/train.csv\", index=True)\n",
    "\n",
    "train_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/30min_history/train.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:59:35.297282Z",
     "start_time": "2023-05-31T00:47:39.951963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"./data/final_miami_datasets/no_history_filled/\",\n",
    "    \"./data/final_miami_datasets/10min_history_filled/\",\n",
    "    \"./data/final_miami_datasets/20min_history_filled/\",\n",
    "    \"./data/final_miami_datasets/30min_history_filled/\"\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in paths:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "train_filled_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/no_history_filled/train.csv\", index=True)\n",
    "train_filled_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/10min_history_filled/train.csv\", index=True)\n",
    "train_filled_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/20min_history_filled/train.csv\", index=True)\n",
    "train_filled_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/30min_history_filled/train.csv\", index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:28:58.475020Z",
     "start_time": "2023-05-31T17:28:57.498503Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"./data/3_miami_val_to_fill.csv\")\n",
    "test = pd.read_csv(\"./data/3_miami_test_to_fill.csv\")\n",
    "\n",
    "val[\"BaseDateTime\"] = pd.to_datetime(val[\"BaseDateTime\"])\n",
    "test[\"BaseDateTime\"] = pd.to_datetime(test[\"BaseDateTime\"])\n",
    "val.set_index([\"voyage_id\", \"BaseDateTime\"], drop=True, inplace=True)\n",
    "test.set_index([\"voyage_id\", \"BaseDateTime\"], drop=True, inplace=True)\n",
    "\n",
    "val_vx = val[\"SOG\"] * 0.5144444 * np.sin(val[\"COG\"])\n",
    "val_vy = val[\"SOG\"] * 0.5144444 * np.cos(val[\"COG\"])\n",
    "val = val.assign(vx=val_vx, vy=val_vy)\n",
    "\n",
    "test_vx = test[\"SOG\"] * 0.5144444 * np.sin(test[\"COG\"])\n",
    "test_vy = test[\"SOG\"] * 0.5144444 * np.cos(test[\"COG\"])\n",
    "test = test.assign(vx=test_vx, vy=test_vy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:28:58.516896Z",
     "start_time": "2023-05-31T17:28:58.473860Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(val.head()[:2])\n",
    "display(test.head()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:29:00.181599Z",
     "start_time": "2023-05-31T17:28:58.506923Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resample to 2 minutes intervals by taking the mean every 2 minutes\n",
    "val_resampled = val.groupby(level=0, sort=False) \\\n",
    "    .resample(\"2T\", level=1).mean()\n",
    "test_resampled = test.groupby(level=0, sort=False) \\\n",
    "    .resample(\"2T\", level=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:29:00.227683Z",
     "start_time": "2023-05-31T17:29:00.182082Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(val_resampled.isna().sum()[0] / val_resampled.shape[0] * 100)\n",
    "print(test_resampled.isna().sum()[0] / test_resampled.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:30:29.669402Z",
     "start_time": "2023-05-31T17:30:27.086752Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_vars_lagged = get_lagged_vars(val_resampled[[\"LON\", \"LAT\",\n",
    "                                                 \"SOG\", \"COG\",\n",
    "                                                 \"distanceToPort\"]],\n",
    "                                  delays=delays,\n",
    "                                  online=True, fill=False, extrapolate=False, quay_point=None)\n",
    "val_nohistory = get_lagged_io(val_vars_lagged,\n",
    "                              val_resampled[\"remainingVoyageTime\"],\n",
    "                              n_feats=5, max_delay=0)\n",
    "val_10min = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=5)\n",
    "val_20min = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=10)\n",
    "val_30min = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vars_lagged = get_lagged_vars(val_resampled[[\"LON\", \"LAT\", \"SOG\", \"COG\", \"distanceToPort\", \"x\", \"y\", \"vx\", \"vy\"]],\n",
    "                                  delays=delays, online=True, fill=True,  extrapolate=False, quay_point=quay_point)\n",
    "val_vars_lagged.index = val_resampled.index\n",
    "val_nohistory_with_outdated_samples = get_lagged_io(val_vars_lagged,\n",
    "                              val_resampled[\"remainingVoyageTime\"],\n",
    "                              n_feats=5, max_delay=0)\n",
    "val_10min_with_outdated_samples = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=5)\n",
    "val_20min_with_outdated_samples = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=10)\n",
    "val_30min_with_outdated_samples = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:30:32.345981Z",
     "start_time": "2023-05-31T17:30:29.677548Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vars_lagged = get_lagged_vars(test_resampled[[\"LON\", \"LAT\",\n",
    "                                                   \"SOG\", \"COG\",\n",
    "                                                   \"distanceToPort\"]],\n",
    "                                   delays=delays,\n",
    "                                   online=True, fill=False, quay_point=None, extrapolate=False)\n",
    "test_nohistory = get_lagged_io(test_vars_lagged,\n",
    "                               test_resampled[\"remainingVoyageTime\"],\n",
    "                               n_feats=5, max_delay=0)\n",
    "test_10min = get_lagged_io(test_vars_lagged,\n",
    "                           test_resampled[\"remainingVoyageTime\"],\n",
    "                           n_feats=5, max_delay=5)\n",
    "test_20min = get_lagged_io(test_vars_lagged,\n",
    "                           test_resampled[\"remainingVoyageTime\"],\n",
    "                           n_feats=5, max_delay=10)\n",
    "test_30min = get_lagged_io(test_vars_lagged,\n",
    "                           test_resampled[\"remainingVoyageTime\"],\n",
    "                           n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:30:47.685448Z",
     "start_time": "2023-05-31T17:30:32.346746Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_vars_lagged_filled = get_lagged_vars(val_resampled[[\"LON\", \"LAT\",\n",
    "                                                        \"SOG\", \"COG\",\n",
    "                                                        \"distanceToPort\", \"x\", \"y\", \"vx\", \"vy\"]],\n",
    "                                         delays=delays,\n",
    "                                         online=True, fill=True, quay_point=quay_point, extrapolate=False)\n",
    "val_vars_lagged_filled.index = val_resampled.index\n",
    "val_nohistory_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                     val_resampled[\"remainingVoyageTime\"],\n",
    "                                     n_feats=5, max_delay=0)\n",
    "val_10min_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                 val_resampled[\"remainingVoyageTime\"],\n",
    "                                 n_feats=5, max_delay=5)\n",
    "val_20min_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                 val_resampled[\"remainingVoyageTime\"],\n",
    "                                 n_feats=5, max_delay=10)\n",
    "val_30min_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                 val_resampled[\"remainingVoyageTime\"],\n",
    "                                 n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:31:02.802252Z",
     "start_time": "2023-05-31T17:30:47.686554Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vars_lagged_filled = get_lagged_vars(test_resampled[[\"LON\", \"LAT\",\n",
    "                                                          \"SOG\", \"COG\",\n",
    "                                                          \"distanceToPort\", \"x\", \"y\", \"vx\", \"vy\"]],\n",
    "                                          delays=delays,\n",
    "                                          online=True, fill=True, quay_point=quay_point, extrapolate=False)\n",
    "test_vars_lagged_filled.index = test_resampled.index\n",
    "test_nohistory_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                      test_resampled[\"remainingVoyageTime\"],\n",
    "                                      n_feats=5, max_delay=0)\n",
    "test_10min_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                  test_resampled[\"remainingVoyageTime\"],\n",
    "                                  n_feats=5, max_delay=5)\n",
    "test_20min_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                  test_resampled[\"remainingVoyageTime\"],\n",
    "                                  n_feats=5, max_delay=10)\n",
    "test_30min_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                  test_resampled[\"remainingVoyageTime\"],\n",
    "                                  n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:31:26.109632Z",
     "start_time": "2023-05-31T17:31:02.800842Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/no_history/val.csv\", index=False)\n",
    "val_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/10min_history/val.csv\", index=False)\n",
    "val_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/20min_history/val.csv\", index=False)\n",
    "val_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/30min_history/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nohistory_with_outdated_samples.dropna().to_csv(\"./data/final_miami_datasets/no_history/val_with_outdated_samples.csv\", index=False)\n",
    "val_10min_with_outdated_samples.dropna().to_csv(\"./data/final_miami_datasets/10min_history/val_with_outdated_samples.csv\", index=False)\n",
    "val_20min_with_outdated_samples.dropna().to_csv(\"./data/final_miami_datasets/20min_history/val_with_outdated_samples.csv\", index=False)\n",
    "val_30min_with_outdated_samples.dropna().to_csv(\"./data/final_miami_datasets/30min_history/val_with_outdated_samples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:31:52.333124Z",
     "start_time": "2023-05-31T17:31:26.109929Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_nohistory_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/no_history_filled/val.csv\", index=False)\n",
    "val_10min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/10min_history_filled/val.csv\", index=False)\n",
    "val_20min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/20min_history_filled/val.csv\", index=False)\n",
    "val_30min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/30min_history_filled/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:32:09.917696Z",
     "start_time": "2023-05-31T17:31:52.332206Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/no_history/test.csv\", index=False)\n",
    "test_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/10min_history/test.csv\", index=False)\n",
    "test_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/20min_history/test.csv\", index=False)\n",
    "test_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/30min_history/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T17:32:29.525992Z",
     "start_time": "2023-05-31T17:32:09.920159Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_nohistory_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/no_history_filled/test.csv\", index=False)\n",
    "test_10min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/10min_history_filled/test.csv\", index=False)\n",
    "test_20min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/20min_history_filled/test.csv\", index=False)\n",
    "test_30min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_miami_datasets/30min_history_filled/test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# France Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set -> Linear Interpolation\n",
    "train = pd.read_csv(\"./data/3_france_train_to_fill.csv\")\n",
    "train[\"BaseDateTime\"] = pd.to_datetime(train[\"BaseDateTime\"])\n",
    "train.set_index([\"voyage_id\", \"BaseDateTime\"], drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 2 minutes intervals by taking the meane very 2 minutes\n",
    "train_resampled = train.groupby(level=0, sort=False)\\\n",
    "            .resample(\"2T\", level=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer.from_crs(4326, 3857)\n",
    "\n",
    "ports = gpd.read_file(\"./data/france_data/Ports_Brittany/port.shp\")\n",
    "brest_port = ports[ports[\"libelle_po\"] == \"Brest\"]\n",
    "lon = brest_port.iloc[0][\"geometry\"].geoms[0].xy[0][0]\n",
    "lat = brest_port.iloc[0][\"geometry\"].geoms[0].xy[1][0]\n",
    "berth_port_coords = [lat, lon-0.01]\n",
    "\n",
    "quay_point = transformer.transform(lat, lon-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = np.arange(1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars_lagged = get_lagged_vars(train_resampled[[\"LON\", \"LAT\",\n",
    "                                                     \"SOG\", \"COG\",\n",
    "                                                     \"distanceToPort\"]],\n",
    "                                    delays=delays,\n",
    "                                    online=False, fill=False, quay_point=None, extrapolate=False)\n",
    "train_nohistory = get_lagged_io(train_vars_lagged,\n",
    "                                train_resampled[\"remainingVoyageTime\"],\n",
    "                                n_feats=5, max_delay=0)\n",
    "train_10min = get_lagged_io(train_vars_lagged,\n",
    "                            train_resampled[\"remainingVoyageTime\"],\n",
    "                            n_feats=5, max_delay=5)\n",
    "train_20min = get_lagged_io(train_vars_lagged,\n",
    "                            train_resampled[\"remainingVoyageTime\"],\n",
    "                            n_feats=5, max_delay=10)\n",
    "train_30min = get_lagged_io(train_vars_lagged,\n",
    "                            train_resampled[\"remainingVoyageTime\"],\n",
    "                            n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars_lagged_filled = get_lagged_vars(train_resampled[[\"LON\", \"LAT\",\n",
    "                                                            \"SOG\", \"COG\",\n",
    "                                                            \"distanceToPort\",\n",
    "                                                            \"x\", \"y\"]],\n",
    "                                           delays=delays,\n",
    "                                           online=False, fill=True, quay_point=quay_point, extrapolate=False)\n",
    "train_filled_nohistory = get_lagged_io(train_vars_lagged_filled,\n",
    "                                       train_resampled[\"remainingVoyageTime\"],\n",
    "                                       n_feats=5, max_delay=0)\n",
    "train_filled_10min = get_lagged_io(train_vars_lagged_filled,\n",
    "                                   train_resampled[\"remainingVoyageTime\"],\n",
    "                                   n_feats=5, max_delay=5)\n",
    "train_filled_20min = get_lagged_io(train_vars_lagged_filled,\n",
    "                                   train_resampled[\"remainingVoyageTime\"],\n",
    "                                   n_feats=5, max_delay=10)\n",
    "train_filled_30min = get_lagged_io(train_vars_lagged_filled,\n",
    "                                   train_resampled[\"remainingVoyageTime\"],\n",
    "                                   n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for each dataset\n",
    "paths = [\n",
    "    \"./data/final_france_datasets/no_history/\",\n",
    "    \"./data/final_france_datasets/10min_history/\",\n",
    "    \"./data/final_france_datasets/20min_history/\",\n",
    "    \"./data/final_france_datasets/30min_history/\"\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in paths:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "train_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/no_history/train.csv\", index=True)\n",
    "train_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/10min_history/train.csv\", index=True)\n",
    "train_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/20min_history/train.csv\", index=True)\n",
    "train_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/30min_history/train.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"./data/final_france_datasets/no_history_filled/\",\n",
    "    \"./data/final_france_datasets/10min_history_filled/\",\n",
    "    \"./data/final_france_datasets/20min_history_filled/\",\n",
    "    \"./data/final_france_datasets/30min_history_filled/\"\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in paths:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "train_filled_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/no_history_filled/train.csv\", index=True)\n",
    "train_filled_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/10min_history_filled/train.csv\", index=True)\n",
    "train_filled_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/20min_history_filled/train.csv\", index=True)\n",
    "train_filled_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/30min_history_filled/train.csv\", index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"./data/3_france_val_to_fill.csv\")\n",
    "test = pd.read_csv(\"./data/3_france_test_to_fill.csv\")\n",
    "\n",
    "val[\"BaseDateTime\"] = pd.to_datetime(val[\"BaseDateTime\"])\n",
    "test[\"BaseDateTime\"] = pd.to_datetime(test[\"BaseDateTime\"])\n",
    "val.set_index([\"voyage_id\", \"BaseDateTime\"], drop=True, inplace=True)\n",
    "test.set_index([\"voyage_id\", \"BaseDateTime\"], drop=True, inplace=True)\n",
    "\n",
    "val_vx = val[\"SOG\"] * 0.5144444 * np.sin(val[\"COG\"])\n",
    "val_vy = val[\"SOG\"] * 0.5144444 * np.cos(val[\"COG\"])\n",
    "val = val.assign(vx=val_vx, vy=val_vy)\n",
    "\n",
    "test_vx = test[\"SOG\"] * 0.5144444 * np.sin(test[\"COG\"])\n",
    "test_vy = test[\"SOG\"] * 0.5144444 * np.cos(test[\"COG\"])\n",
    "test = test.assign(vx=test_vx, vy=test_vy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 2 minutes intervals by taking the mean every 2 minutes\n",
    "val_resampled = val.groupby(level=0, sort=False) \\\n",
    "    .resample(\"2T\", level=1).mean()\n",
    "test_resampled = test.groupby(level=0, sort=False) \\\n",
    "    .resample(\"2T\", level=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_resampled.isna().sum()[0] / val_resampled.shape[0] * 100)\n",
    "print(test_resampled.isna().sum()[0] / test_resampled.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vars_lagged = get_lagged_vars(val_resampled[[\"LON\", \"LAT\",\n",
    "                                                 \"SOG\", \"COG\",\n",
    "                                                 \"distanceToPort\"]],\n",
    "                                  delays=delays,\n",
    "                                  online=True, fill=False, quay_point=None, extrapolate=False)\n",
    "val_nohistory = get_lagged_io(val_vars_lagged,\n",
    "                              val_resampled[\"remainingVoyageTime\"],\n",
    "                              n_feats=5, max_delay=0)\n",
    "val_10min = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=5)\n",
    "val_20min = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=10)\n",
    "val_30min = get_lagged_io(val_vars_lagged,\n",
    "                          val_resampled[\"remainingVoyageTime\"],\n",
    "                          n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vars_lagged = get_lagged_vars(test_resampled[[\"LON\", \"LAT\",\n",
    "                                                   \"SOG\", \"COG\",\n",
    "                                                   \"distanceToPort\"]],\n",
    "                                   delays=delays,\n",
    "                                   online=True, fill=False, quay_point=None, extrapolate=False)\n",
    "test_nohistory = get_lagged_io(test_vars_lagged,\n",
    "                               test_resampled[\"remainingVoyageTime\"],\n",
    "                               n_feats=5, max_delay=0)\n",
    "test_10min = get_lagged_io(test_vars_lagged,\n",
    "                           test_resampled[\"remainingVoyageTime\"],\n",
    "                           n_feats=5, max_delay=5)\n",
    "test_20min = get_lagged_io(test_vars_lagged,\n",
    "                           test_resampled[\"remainingVoyageTime\"],\n",
    "                           n_feats=5, max_delay=10)\n",
    "test_30min = get_lagged_io(test_vars_lagged,\n",
    "                           test_resampled[\"remainingVoyageTime\"],\n",
    "                           n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vars_lagged_filled = get_lagged_vars(val_resampled[[\"LON\", \"LAT\",\n",
    "                                                        \"SOG\", \"COG\",\n",
    "                                                        \"distanceToPort\", \"x\", \"y\", \"vx\", \"vy\"]],\n",
    "                                         delays=delays,\n",
    "                                         online=True, fill=True, quay_point=quay_point, extrapolate=False)\n",
    "val_vars_lagged_filled.index = val_resampled.index\n",
    "val_nohistory_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                     val_resampled[\"remainingVoyageTime\"],\n",
    "                                     n_feats=5, max_delay=0)\n",
    "val_10min_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                 val_resampled[\"remainingVoyageTime\"],\n",
    "                                 n_feats=5, max_delay=5)\n",
    "val_20min_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                 val_resampled[\"remainingVoyageTime\"],\n",
    "                                 n_feats=5, max_delay=10)\n",
    "val_30min_filled = get_lagged_io(val_vars_lagged_filled,\n",
    "                                 val_resampled[\"remainingVoyageTime\"],\n",
    "                                 n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vars_lagged_filled = get_lagged_vars(test_resampled[[\"LON\", \"LAT\",\n",
    "                                                          \"SOG\", \"COG\",\n",
    "                                                          \"distanceToPort\", \"x\", \"y\", \"vx\", \"vy\"]],\n",
    "                                          delays=delays,\n",
    "                                          online=True, fill=True, quay_point=quay_point, extrapolate=False)\n",
    "test_vars_lagged_filled.index = test_resampled.index\n",
    "test_nohistory_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                      test_resampled[\"remainingVoyageTime\"],\n",
    "                                      n_feats=5, max_delay=0)\n",
    "test_10min_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                  test_resampled[\"remainingVoyageTime\"],\n",
    "                                  n_feats=5, max_delay=5)\n",
    "test_20min_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                  test_resampled[\"remainingVoyageTime\"],\n",
    "                                  n_feats=5, max_delay=10)\n",
    "test_30min_filled = get_lagged_io(test_vars_lagged_filled,\n",
    "                                  test_resampled[\"remainingVoyageTime\"],\n",
    "                                  n_feats=5, max_delay=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/no_history/val.csv\", index=False)\n",
    "val_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/10min_history/val.csv\", index=False)\n",
    "val_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/20min_history/val.csv\", index=False)\n",
    "val_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/30min_history/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nohistory_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/no_history_filled/val.csv\", index=False)\n",
    "val_10min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/10min_history_filled/val.csv\", index=False)\n",
    "val_20min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/20min_history_filled/val.csv\", index=False)\n",
    "val_30min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/30min_history_filled/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nohistory.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/no_history/test.csv\", index=False)\n",
    "test_10min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/10min_history/test.csv\", index=False)\n",
    "test_20min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/20min_history/test.csv\", index=False)\n",
    "test_30min.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/30min_history/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nohistory_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/no_history_filled/test.csv\", index=False)\n",
    "test_10min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/10min_history_filled/test.csv\", index=False)\n",
    "test_20min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/20min_history_filled/test.csv\", index=False)\n",
    "test_30min_filled.dropna()\\\n",
    "    .to_csv(\"./data/final_france_datasets/30min_history_filled/test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plots and Tables of Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miami_nohist = pd.read_csv(\"./data/final_miami_datasets/no_history/train.csv\")\n",
    "val_miami_nohist = pd.read_csv(\"./data/final_miami_datasets/no_history/val.csv\")\n",
    "test_miami_nohist = pd.read_csv(\"./data/final_miami_datasets/no_history/test.csv\")\n",
    "\n",
    "train_miami_nohist_filled = pd.read_csv(\"./data/final_miami_datasets/no_history_filled/train.csv\")\n",
    "val_miami_nohist_filled = pd.read_csv(\"./data/final_miami_datasets/no_history_filled/val.csv\")\n",
    "test_miami_nohist_filled = pd.read_csv(\"./data/final_miami_datasets/no_history_filled/test.csv\")\n",
    "\n",
    "train_france_nohist = pd.read_csv(\"./data/final_france_datasets/no_history/train.csv\")\n",
    "val_france_nohist = pd.read_csv(\"./data/final_france_datasets/no_history/val.csv\")\n",
    "test_france_nohist = pd.read_csv(\"./data/final_france_datasets/no_history/test.csv\")\n",
    "\n",
    "train_france_nohist_filled = pd.read_csv(\"./data/final_france_datasets/no_history_filled/train.csv\")\n",
    "val_france_nohist_filled = pd.read_csv(\"./data/final_france_datasets/no_history_filled/val.csv\")\n",
    "test_france_nohist_filled = pd.read_csv(\"./data/final_france_datasets/no_history_filled/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer bins de tempos restantes de viagem\n",
    "bins = np.array([0.0, 8.0, 16.0, 24.0, 32.0, 40.0, 48.0]) * 60\n",
    "labels= [\"0-8\", \"8-16\", \"16-24\", \"24-32\", \"32-40\", \"40-48\"]\n",
    "\n",
    "train_miami_nohist_binned = pd.cut(x=train_miami_nohist[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_miami_nohist_binned = pd.cut(x=val_miami_nohist[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "test_miami_nohist_binned = pd.cut(x=test_miami_nohist[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "\n",
    "train_miami_nohist_filled_binned = pd.cut(x=train_miami_nohist_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_miami_nohist_filled_binned = pd.cut(x=val_miami_nohist_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "test_miami_nohist_filled_binned = pd.cut(x=test_miami_nohist_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "\n",
    "train_france_nohist_binned = pd.cut(x=train_france_nohist[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_france_nohist_binned = pd.cut(x=val_france_nohist[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "test_france_nohist_binned = pd.cut(x=test_france_nohist[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "\n",
    "train_france_nohist_filled_binned = pd.cut(x=train_france_nohist_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_france_nohist_filled_binned = pd.cut(x=val_france_nohist_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "test_france_nohist_filled_binned = pd.cut(x=test_france_nohist_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miami_nohist_toplot = pd.concat((train_miami_nohist_binned, train_miami_nohist_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_miami_nohist_toplot = pd.concat((val_miami_nohist_binned, val_miami_nohist_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "test_miami_nohist_toplot = pd.concat((test_miami_nohist_binned, test_miami_nohist_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "train_france_nohist_toplot = pd.concat((train_france_nohist_binned, train_france_nohist_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_france_nohist_toplot = pd.concat((val_france_nohist_binned, val_france_nohist_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "test_france_nohist_toplot = pd.concat((test_france_nohist_binned, test_france_nohist_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots dos sets de treino val e teste sem historico para ambos os datasets\n",
    "_, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "sns.barplot(data=train_miami_nohist_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[0])\n",
    "sns.barplot(data=val_miami_nohist_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[1])\n",
    "sns.barplot(data=test_miami_nohist_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[2])\n",
    "\n",
    "axs[0].set_title(\"Train Set\")\n",
    "axs[1].set_title(\"Validation Set\")\n",
    "axs[2].set_title(\"Test Set\")\n",
    "\n",
    "axs[0].set_ylabel(\"Number of Samples\")\n",
    "axs[0].set_xlabel(\"Remaining Travel Time (h)\")\n",
    "\n",
    "axs[1].set_ylabel(\"\")\n",
    "axs[2].set_ylabel(\"\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[2].set_xlabel(\"\")\n",
    "\n",
    "\n",
    "plt.savefig(\"./figures/fill_miami_nohist_bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots dos sets de treino val e teste sem historico para ambos os datasets\n",
    "_, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "sns.barplot(data=train_france_nohist_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[0])\n",
    "sns.barplot(data=val_france_nohist_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[1])\n",
    "sns.barplot(data=test_france_nohist_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[2])\n",
    "\n",
    "axs[0].set_title(\"Train Set\")\n",
    "axs[1].set_title(\"Validation Set\")\n",
    "axs[2].set_title(\"Test Set\")\n",
    "\n",
    "axs[0].set_ylabel(\"Number of Samples\")\n",
    "axs[0].set_xlabel(\"Remaining Travel Time (h)\")\n",
    "\n",
    "axs[1].set_ylabel(\"\")\n",
    "axs[2].set_ylabel(\"\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[2].set_xlabel(\"\")\n",
    "\n",
    "\n",
    "plt.savefig(\"./figures/fill_france_nohist_bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots do set de val para os 3 historicos em ambos os datasets\n",
    "val_miami_10min = pd.read_csv(\"./data/final_miami_datasets/10min_history/val.csv\")\n",
    "val_miami_20min = pd.read_csv(\"./data/final_miami_datasets/20min_history/val.csv\")\n",
    "val_miami_30min = pd.read_csv(\"./data/final_miami_datasets/30min_history/val.csv\")\n",
    "\n",
    "val_miami_10min_filled = pd.read_csv(\"./data/final_miami_datasets/10min_history_filled/val.csv\")\n",
    "val_miami_20min_filled = pd.read_csv(\"./data/final_miami_datasets/20min_history_filled/val.csv\")\n",
    "val_miami_30min_filled = pd.read_csv(\"./data/final_miami_datasets/30min_history_filled/val.csv\")\n",
    "\n",
    "val_france_10min = pd.read_csv(\"./data/final_france_datasets/10min_history/val.csv\")\n",
    "val_france_20min = pd.read_csv(\"./data/final_france_datasets/20min_history/val.csv\")\n",
    "val_france_30min = pd.read_csv(\"./data/final_france_datasets/30min_history/val.csv\")\n",
    "\n",
    "val_france_10min_filled = pd.read_csv(\"./data/final_france_datasets/10min_history_filled/val.csv\")\n",
    "val_france_20min_filled = pd.read_csv(\"./data/final_france_datasets/20min_history_filled/val.csv\")\n",
    "val_france_30min_filled = pd.read_csv(\"./data/final_france_datasets/30min_history_filled/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer bins de tempos restantes de viagem\n",
    "bins = np.array([0.0, 8.0, 16.0, 24.0, 32.0, 40.0, 48.0]) * 60\n",
    "labels= [\"0-8\", \"8-16\", \"16-24\", \"24-32\", \"32-40\", \"40-48\"]\n",
    "\n",
    "val_miami_10min_binned = pd.cut(x=val_miami_10min[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_miami_20min_binned = pd.cut(x=val_miami_20min[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_miami_30min_binned = pd.cut(x=val_miami_30min[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "\n",
    "val_miami_10min_filled_binned = pd.cut(x=val_miami_10min_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_miami_20min_filled_binned = pd.cut(x=val_miami_20min_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_miami_30min_filled_binned = pd.cut(x=val_miami_30min_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "\n",
    "val_france_10min_binned = pd.cut(x=val_france_10min[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_france_20min_binned = pd.cut(x=val_france_20min[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_france_30min_binned = pd.cut(x=val_france_30min[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "\n",
    "val_france_10min_filled_binned = pd.cut(x=val_france_10min_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_france_20min_filled_binned = pd.cut(x=val_france_20min_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()\n",
    "val_france_30min_filled_binned = pd.cut(x=val_france_30min_filled[\"remainingVoyageTime\"], bins=bins, labels=labels)\\\n",
    "    .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_miami_10min_toplot = pd.concat((val_miami_10min_binned, val_miami_10min_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_miami_20min_toplot = pd.concat((val_miami_20min_binned, val_miami_20min_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_miami_30min_toplot = pd.concat((val_miami_30min_binned, val_miami_30min_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_france_10min_toplot = pd.concat((val_france_10min_binned, val_france_10min_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_france_20min_toplot = pd.concat((val_france_20min_binned, val_france_20min_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()\n",
    "\n",
    "val_france_30min_toplot = pd.concat((val_france_30min_binned, val_france_30min_filled_binned),\n",
    "                                    axis=1, keys=[\"No Fill\", \"Filled\"])\\\n",
    "    .melt(var_name=\"FillType\", value_name=\"Count\", ignore_index=False)\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots dos sets de treino val e teste sem historico para ambos os datasets\n",
    "_, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "sns.barplot(data=val_miami_10min_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[0])\n",
    "sns.barplot(data=val_miami_20min_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[1])\n",
    "sns.barplot(data=val_miami_30min_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[2])\n",
    "\n",
    "axs[0].set_title(\"10 Minnutes\")\n",
    "axs[1].set_title(\"20 Minutes\")\n",
    "axs[2].set_title(\"30 Minutes\")\n",
    "\n",
    "axs[0].set_ylabel(\"Number of Samples\")\n",
    "axs[0].set_xlabel(\"Remaining Travel Time (h)\")\n",
    "\n",
    "axs[1].set_ylabel(\"\")\n",
    "axs[2].set_ylabel(\"\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[2].set_xlabel(\"\")\n",
    "\n",
    "\n",
    "plt.savefig(\"./figures/fill_miami_val_hist_bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots dos sets de treino val e teste sem historico para ambos os datasets\n",
    "_, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "sns.barplot(data=val_france_10min_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[0])\n",
    "sns.barplot(data=val_france_20min_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[1])\n",
    "sns.barplot(data=val_france_30min_toplot, x=\"remainingVoyageTime\", y=\"Count\", hue=\"FillType\", ax=axs[2])\n",
    "\n",
    "axs[0].set_title(\"10 Minnutes\")\n",
    "axs[1].set_title(\"20 Minutes\")\n",
    "axs[2].set_title(\"30 Minutes\")\n",
    "\n",
    "axs[0].set_ylabel(\"Number of Samples\")\n",
    "axs[0].set_xlabel(\"Remaining Travel Time (h)\")\n",
    "\n",
    "axs[1].set_ylabel(\"\")\n",
    "axs[2].set_ylabel(\"\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[2].set_xlabel(\"\")\n",
    "\n",
    "\n",
    "plt.savefig(\"./figures/fill_france_val_hist_bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
